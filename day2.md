# ðŸ“… Day 2 â€“ Market Data Ingestion & OHLCV Understanding

## ðŸŽ¯ Goal of Day 2

The objective of Day 2 was to **understand and implement market data ingestion**, specifically:

* What **OHLCV data** is
* How raw market data becomes **structured time-series data**
* How to fetch, store, and reason about this data programmatically
* How this fits into a **quantitative trading system**

This is the **foundation layer** of any quant / trading platform.

---

## ðŸ§  Big Picture (Plain English)

Before trading, predicting, or backtesting:

> **We must first answer:**
> â€œWhat actually happened in the market, at each point in time?â€

Day 2 is about **capturing reality** in a format machines can analyze.

---

## ðŸ“Š What is OHLCV?

**OHLCV** represents market price movement for a given time interval.

| Term       | Meaning                    | Example                    |
| ---------- | -------------------------- | -------------------------- |
| **Open**   | Price at start of interval | 9:15 AM price              |
| **High**   | Highest price in interval  | Dayâ€™s peak                 |
| **Low**    | Lowest price in interval   | Dayâ€™s bottom               |
| **Close**  | Price at end of interval   | 9:30 AM price              |
| **Volume** | Quantity traded            | Number of shares/contracts |

### Example (1-minute candle)

```text
09:15 â€“ 09:16
Open: 100
High: 102
Low: 99
Close: 101
Volume: 12,000
```

This single row summarizes **hundreds or thousands of trades**.

---

## â± What Does â€œIntervalâ€ Mean?

The **interval** defines *how we compress time*.

| Interval | Meaning                  |
| -------- | ------------------------ |
| `1m`     | One candle per minute    |
| `5m`     | One candle per 5 minutes |
| `1h`     | One candle per hour      |
| `1d`     | One candle per day       |

ðŸ“Œ **Why interval matters:**
Strategies behave *very differently* on 1-minute vs daily data.

---

## ðŸ”„ What Are We Doing in Code?

### Step 1: Fetch Raw Market Data

We call a function like:

```python
fetch_ohlcv(symbol, interval)
```

Conceptually:

> â€œGive me historical price movement for **this asset**, grouped by **this time window**.â€

---

### Step 2: Convert to Structured Table

The raw API data is converted into a **tabular time-series**:

```text
timestamp | open | high | low | close | volume
```

Why this matters:

* Machines think in **tables**
* Quants think in **time-indexed matrices**
* kdb+/qdb+ is *built exactly for this structure*

---

### Step 3: Store as `ohlcv.parquet`

#### What is `ohlcv.parquet`?

`parquet` is a **columnar storage format**, optimized for analytics.

Think of it as:

* CSV âŒ slow, large
* JSON âŒ messy
* Parquet âœ… fast, compressed, analytics-friendly

ðŸ“Œ **Why Parquet is important for quants:**

* Faster reads
* Less memory
* Perfect for time-series analytics
* Common in institutional pipelines

---

## ðŸ§© Why This Matters for kdb+ / qdb+

kdb+ is designed for **exactly this type of data**:

```text
time | symbol | open | high | low | close | volume
```

Day 2 prepares us to later:

* Load OHLCV into kdb+
* Run ultra-fast time-series queries
* Compare Python vs q performance
* Simulate real trading desks

---

## âš ï¸ Error Encountered & Learning

### Error:

```text
TypeError: fetch_ohlcv() missing 1 required positional argument: 'interval'
```

### What it taught:

* Functions require **all mandatory parameters**
* Market data **cannot exist without time granularity**
* â€œPriceâ€ alone is meaningless without **when**

This reinforced the mental model:

> **Time is the backbone of quantitative systems**

---

## ðŸ§  Mental Model (Important)

By end of Day 2, the system looks like this:

```
Market Exchange
     â†“
Raw Trades
     â†“
OHLCV Aggregation (by interval)
     â†“
Structured Table
     â†“
Parquet Storage
     â†“
Future Analytics / kdb+ / Strategies
```

Everything later (signals, ML, alpha, execution) **depends on this layer**.

---

## âœ… What We Achieved Today

âœ” Understood OHLCV deeply
âœ” Learned why interval is mandatory
âœ” Built first market data ingestion pipeline
âœ” Stored time-series data properly
âœ” Built quant-grade mental models
âœ” Prepared ground for kdb+ integration

---
